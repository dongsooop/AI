import requests
from bs4 import BeautifulSoup
import csv

# URL ì„¤ì •
BASE_URL = "https://www.dongyang.ac.kr"
PAGE_URL = "https://www.dongyang.ac.kr/dmu/4904/subview.do?page="

# í˜ì´ì§€ í¬ë¡¤ë§
def crawl_page(page):
    url = PAGE_URL + str(page)
    response = requests.get(url)
    response.encoding = "utf-8"
    soup = BeautifulSoup(response.text, "html.parser")
    return soup

# í˜ì´ì§€ ë‚´ìš© íŒŒì‹±
def parse_page(soup):
    rows = soup.select("table.board-table tbody tr")
    data = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 7:
            continue

        num = cols[0].text.strip()
        title_tag = cols[1].find("a")
        title = title_tag.get_text(strip=True) if title_tag else ""
        link = BASE_URL + title_tag['href'] if title_tag else ""

        department = cols[2].text.strip()
        writer = cols[3].text.strip()
        date = cols[4].text.strip()
        views = cols[5].text.strip()
        files = cols[6].text.strip()

        data.append([num, title, department, writer, date, views, files, link])
    return data

# CSV íŒŒì¼ë¡œ ì €ì¥
def save_to_csv(data, filename="data/dongyang_notices.csv"):
    with open(filename, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["ë²ˆí˜¸", "ì œëª©", "ë¶€ì„œ", "ì‘ì„±ì", "ì‘ì„±ì¼", "ì¡°íšŒìˆ˜", "ì²¨ë¶€íŒŒì¼ ìˆ˜", "ë§í¬"])
        writer.writerows(data)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
    remove_notice_rows("data/dongyang_notices.csv")

# 'ê³µì§€' í•­ëª© ì œê±°
def remove_notice_rows(input_file, output_file=None):
    if output_file is None:
        output_file = input_file  # ë®ì–´ì“°ê¸°

    filtered_rows = []

    with open(input_file, mode="r", encoding="utf-8-sig") as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            if row and row[0].isdigit():  # ë²ˆí˜¸ê°€ ìˆ«ìì¸ ê²½ìš°ë§Œ ì €ì¥
                filtered_rows.append(row)

    with open(output_file, mode="w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(filtered_rows)

    print(f"âœ… 'ê³µì§€' í•­ëª© ì œê±° ì™„ë£Œ â†’ {output_file}")

# ë©”ì¸
def main():
    all_data = []
    for page in range(1, 487):  # 1~5í˜ì´ì§€
        print(f"ğŸ“„ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
        soup = crawl_page(page)
        page_data = parse_page(soup)
        all_data.extend(page_data)

    save_to_csv(all_data)

# ì‹¤í–‰
if __name__ == "__main__":
    main()