import requests
from bs4 import BeautifulSoup
import csv

BASE_URL = "https://www.dongyang.ac.kr"
LIST_URL = f"{BASE_URL}/bbs/dmu/677/artclList.do"

all_data = []

def fetch_page(page_index):
    data = {
        "pageIndex": page_index,
        "menuSeq": "677",
        "boardSeq": "677"
    }
    response = requests.post(LIST_URL, data=data)
    response.encoding = 'utf-8'
    return BeautifulSoup(response.text, "html.parser")

def parse_notices(soup):
    rows = soup.select("table.board-table tbody tr")
    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 7:
            continue

        num = cols[0].get_text(strip=True)
        title_tag = cols[1].find("a")
        title = title_tag.get_text(strip=True) if title_tag else ""
        link = BASE_URL + title_tag['href'] if title_tag else ""

        department = cols[2].get_text(strip=True)
        writer = cols[3].get_text(strip=True)
        date = cols[4].get_text(strip=True)
        views = cols[5].get_text(strip=True)
        files = cols[6].get_text(strip=True)

        print(f"ðŸ“Œ [{num}] {title}")
        print(f"   ðŸ¢ ë¶€ì„œ: {department} | âœ ìž‘ì„±ìž: {writer} | ðŸ“… ìž‘ì„±ì¼: {date}")
        print(f"   ðŸ‘ ì¡°íšŒìˆ˜: {views} | ðŸ“Ž ì²¨ë¶€íŒŒì¼: {files}")
        print(f"   ðŸ”— ë§í¬: {link}\n")

        all_data.append([num, title, department, writer, date, views, files, link])

def save_to_csv(filename="data/dongyang_notices.csv"):
    with open(filename, mode='w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["ë²ˆí˜¸", "ì œëª©", "ë¶€ì„œ", "ìž‘ì„±ìž", "ìž‘ì„±ì¼", "ì¡°íšŒìˆ˜", "ì²¨ë¶€íŒŒì¼ ìˆ˜", "ë§í¬"])
        writer.writerows(all_data)
    print(f"\nâœ… CSV íŒŒì¼ ì €ìž¥ ì™„ë£Œ: {filename}")
    remove_duplicates_from_csv()

def crawl_all_pages(start=1, end=3):
    for page in range(start, end + 1):
        print(f"\n===== âœ… {page} íŽ˜ì´ì§€ =====")
        soup = fetch_page(page)
        parse_notices(soup)

    save_to_csv()

def remove_duplicates_from_csv(input_file="data/dongyang_notices.csv", output_file="data/dongyang_notices.csv"):
    seen = set()
    unique_rows = []

    with open(input_file, mode="r", encoding="utf-8-sig") as f:
        reader = csv.reader(f)
        header = next(reader)  # í—¤ë”ëŠ” ë”°ë¡œ ì €ìž¥
        for row in reader:
            # ë²ˆí˜¸ + ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ íŒë‹¨
            key = (row[0], row[1])
            if key not in seen:
                seen.add(key)
                unique_rows.append(row)

    with open(output_file, mode="w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(unique_rows)

    print(f"\nâœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ! â†’ {output_file} ì— ì €ìž¥ë¨.")

# ì‹¤í–‰
if __name__ == "__main__":
    crawl_all_pages(start=1, end=3)  # íŽ˜ì´ì§€ ë²”ìœ„ëŠ” í•„ìš”ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥