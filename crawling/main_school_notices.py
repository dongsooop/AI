import requests
from bs4 import BeautifulSoup
import csv
import os

# URL ì„¤ì •
BASE_URL = "https://www.dongyang.ac.kr"

# fnctNo ìë™ ì¶”ì¶œ í•¨ìˆ˜
def extract_fnctno_from_subview(url):
    response = requests.get(url)
    response.encoding = "utf-8"
    soup = BeautifulSoup(response.text, "html.parser")
    form = soup.select_one("form[action*='/combBbs/']")
    if form:
        action_url = form.get("action", "")
        if "/combBbs/" in action_url and "/list.do" in action_url:
            fnct_no = action_url.split("/")[3]  # /combBbs/dmu/84/list.do -> 84
            return fnct_no
    return None

# department.txt ì½ê¸° ë° fnctNo ì¶”ì¶œ
def load_departments(file_path="data/department.txt"):
    departments = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {line.strip()}")  # â† ë¡œê·¸ ì¶”ê°€
            parts = line.strip().split(",")
            if len(parts) == 3:
                name, subview_url, page_count = parts
                subview_full_url = BASE_URL + subview_url
                
                if "í•™êµê³µì§€" in name:  # âœ… í•™êµê³µì§€ ì˜ˆì™¸ ì²˜ë¦¬
                    print(f"ğŸ“Œ {name}: í•™êµ ê³µì§€ì´ë¯€ë¡œ fnctNo ì¶”ì¶œ ìƒëµ")
                    departments.append((name, subview_url + "?page=", int(page_count), None))
                    continue

                fnct_no = extract_fnctno_from_subview(subview_full_url)
                print(f"ğŸ” {name}: ì¶”ì¶œëœ fnctNo = {fnct_no}")  # â† ë¡œê·¸ ì¶”ê°€
                if fnct_no:
                    combbbs_url = f"/combBbs/dmu/{fnct_no}/list.do?page="
                    departments.append((name, combbbs_url, int(page_count), fnct_no))
    return departments

# í˜ì´ì§€ í¬ë¡¤ë§
def crawl_page(url):
    response = requests.get(url)
    response.encoding = "utf-8"
    soup = BeautifulSoup(response.text, "html.parser")
    return soup

# í•™êµ ê³µì§€ í˜ì´ì§€ ë‚´ìš© íŒŒì‹±
def parse_page(soup):
    rows = soup.select("table.board-table tbody tr")
    data = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 7:
            continue
        num = cols[0].text.strip()
        title_tag = cols[1].find("a")
        title = title_tag.get_text(strip=True) if title_tag else ""
        link = BASE_URL + title_tag['href'] if title_tag else ""
        department = cols[2].text.strip()
        writer = cols[3].text.strip()
        date = cols[4].text.strip()
        data.append([num, title, department, writer, date, link])
    return data

# í•™ê³¼ê³µì§€ í˜ì´ì§€ ë‚´ìš© íŒŒì‹± (fnctNo í¬í•¨)
def parse_department_page(soup, fnct_no):
    rows = soup.select("table.board-table.horizon tbody tr")
    data = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 4:
            continue
        num = cols[0].text.strip()
        title_tag = cols[1].find("a")
        title = title_tag.get_text(strip=True) if title_tag else ""
        onclick = title_tag.get("href", "")
        nttId = None
        if "jf_combBbs_view" in onclick:
            try:
                parts = onclick.split(",")
                nttId = parts[-1].strip("'); ")
                link = f"https://www.dongyang.ac.kr/combBbs/dmu/{fnct_no}/view.do?nttId={nttId}"
            except:
                link = ""
        else:
            link = ""
        writer = cols[2].text.strip()
        date = cols[3].text.strip()
        department = "í•™ê³¼ê³µì§€"
        data.append([num, title, department, writer, date, link])
    return data

# CSV íŒŒì¼ë¡œ ì €ì¥
def save_to_csv(data, department_name):
    filename = f"data/dept/{department_name}_notices.csv"
    with open(filename, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["ë²ˆí˜¸", "ì œëª©", "ë¶€ì„œ", "ì‘ì„±ì", "ì‘ì„±ì¼", "ë§í¬"])
        writer.writerows(data)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
    remove_notice_rows(filename)

# 'ê³µì§€' í•­ëª© ì œê±°
def remove_notice_rows(input_file, output_file=None):
    if output_file is None:
        output_file = input_file
    filtered_rows = []
    with open(input_file, mode="r", encoding="utf-8-sig") as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            if row and row[0].isdigit():
                filtered_rows.append(row)
    with open(output_file, mode="w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(filtered_rows)
    print(f"âœ… 'ê³µì§€' í•­ëª© ì œê±° ì™„ë£Œ â†’ {output_file}")

# ë©”ì¸ ì‹¤í–‰
def main():
    departments = load_departments()
    for dept_name, dept_url, page_count, fnct_no in departments:
        print(f"\nğŸ“š {dept_name} í¬ë¡¤ë§ ì‹œì‘ ({page_count} í˜ì´ì§€)...")
        all_data = []
        for page in range(1, page_count + 1):
            print(f"ğŸ“„ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
            full_url = BASE_URL + dept_url + str(page)
            soup = crawl_page(full_url)
            if "combBbs" in dept_url:
                page_data = parse_department_page(soup, fnct_no)
            else:
                page_data = parse_page(soup)
            all_data.extend(page_data)
        save_to_csv(all_data, dept_name)

if __name__ == "__main__":
    main()
