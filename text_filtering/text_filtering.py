from fastapi import FastAPI
from pydantic import BaseModel
from transformers import ElectraTokenizer, ElectraForSequenceClassification
import torch
import os
from typing import Tuple

# FastAPI ì•± ìƒì„±
app = FastAPI()

# ìš”ì²­ ë°”ë”” ì •ì˜
class TextRequest(BaseModel):
    text: str

# ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "..", "model", "my_electra_finetuned")
LOG_PATH = os.path.join(BASE_DIR, "..", "data", "bad_text_sample.txt")
# AWSì „ìš© PATHìˆ˜ì •
# LOG_PATH = "/app/data/bad_text_sample.txt"

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ë¡œì»¬ì—ì„œ)
tokenizer = ElectraTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)
model = ElectraForSequenceClassification.from_pretrained(MODEL_PATH, local_files_only=True)

# ì¥ë¹„ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# ì˜ˆì¸¡ í•¨ìˆ˜ (label ìˆ«ìì™€ í…ìŠ¤íŠ¸ ë°˜í™˜)
def predict(text: str) -> Tuple[int, str]:
    encoded = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=64,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        pred_label = torch.argmax(logits, dim=-1).item()

    return pred_label, "ë¹„ì†ì–´" if pred_label == 1 else "ì •ìƒ"

# API ì—”ë“œí¬ì¸íŠ¸
@app.post("/predict")
async def classify_text(request: TextRequest):
    text = request.text.strip()
    label_num, label_text = predict(text)

    # ğŸ”¥ ì €ì¥: í…ìŠ¤íŠ¸|0 ë˜ëŠ” í…ìŠ¤íŠ¸|1
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(f"{text}|{label_num}\n")

    return {"label": label_text}