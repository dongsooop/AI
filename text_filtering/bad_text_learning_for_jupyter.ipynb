{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f2824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 데이터 개수: 5846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl_study/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 학습 세트: 4676 | 검증 세트: 1170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl_study/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.4886 | Val Acc: 0.8504\n",
      "[Epoch 2] Train Loss: 0.2977 | Val Acc: 0.8581\n",
      "[Epoch 3] Train Loss: 0.1883 | Val Acc: 0.8581\n",
      "[Epoch 4] Train Loss: 0.1110 | Val Acc: 0.8538\n",
      "[Epoch 5] Train Loss: 0.0698 | Val Acc: 0.8521\n",
      "[Epoch 6] Train Loss: 0.0587 | Val Acc: 0.8487\n",
      "[Epoch 7] Train Loss: 0.0558 | Val Acc: 0.8590\n",
      "[Epoch 8] Train Loss: 0.0421 | Val Acc: 0.8667\n",
      "[Epoch 9] Train Loss: 0.0359 | Val Acc: 0.8632\n",
      "[Epoch 10] Train Loss: 0.0232 | Val Acc: 0.8607\n",
      "[결과] '애새끼가 초딩도 아니고 ㅋㅋㅋㅋ' => 비속어 판정\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "##########################\n",
    "# 1) 전처리 함수\n",
    "##########################\n",
    "okt = Okt()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 예시: 불필요한 특수문자 제거\n",
    "    text = re.sub(r\"[^가-힣0-9A-Za-z\\s\\.?!]\", \"\", text).strip()\n",
    "    # 형태소 분석 -> 토큰 리스트\n",
    "    tokens = okt.morphs(text)\n",
    "    # 토큰들을 공백으로 연결 (또는 그대로 리스트로 두어도 무방)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "##########################\n",
    "# 2) Dataset 정의\n",
    "##########################\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, data_list, tokenizer, max_len=64):\n",
    "        \"\"\"\n",
    "        data_list: [(문장, 라벨), (문장, 라벨), ...]\n",
    "        tokenizer: KoELECTRA 전처리용 tokenizer\n",
    "        max_len: 최대 토큰 길이\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        # 전처리 (형태소 분석 등)\n",
    "        proc_text = preprocess_text(text)\n",
    "        \n",
    "        # KoELECTRA 토크나이저\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            proc_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 텐서 형식으로 반환\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "##########################\n",
    "# 3) 학습/검증 루프 정의\n",
    "##########################\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def eval_one_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            logits = outputs.logits  # shape: (batch_size, 2)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "\n",
    "##########################\n",
    "# 4) 메인 실행부\n",
    "##########################\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- 4.1) 데이터 불러오기 (문장|라벨)\n",
    "    train_file = \"../data/bad_text_sample.txt\"  # 실제 경로에 맞춰 수정\n",
    "    data_list = []\n",
    "    \n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # \"문장|라벨\" 형태이므로, 뒷부분만 분리\n",
    "            parts = line.rsplit(\"|\", 1)\n",
    "            text = parts[0].strip()\n",
    "            label = int(parts[1].strip())\n",
    "            data_list.append((text, label))\n",
    "    \n",
    "    print(f\"[INFO] 데이터 개수: {len(data_list)}\")\n",
    "    \n",
    "    # ----- 4.2) Tokenizer & Dataset\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "    dataset = CommentDataset(data_list, tokenizer, max_len=64)\n",
    "    \n",
    "    # ----- 4.3) Train/Validation 분할\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    print(f\"[INFO] 학습 세트: {len(train_dataset)} | 검증 세트: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # ----- 4.4) 모델 초기화\n",
    "    model = ElectraForSequenceClassification.from_pretrained(\n",
    "        \"monologg/koelectra-base-v3-discriminator\",\n",
    "        num_labels=2  # 비속어(1), 정상(0) 이진 분류\n",
    "    )\n",
    "    \n",
    "    # GPU/CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # 옵티마이저, 에폭 설정\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    epochs = 10\n",
    "    \n",
    "    # ----- 4.5) 학습\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_acc = eval_one_epoch(model, val_loader, device)\n",
    "        print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # ----- 4.6) 학습된 모델 저장\n",
    "    model.save_pretrained(\n",
    "    \"model/my_electra_finetuned\",\n",
    "    safe_serialization=False\n",
    "    )\n",
    "    tokenizer.save_pretrained(\"../model/my_electra_finetuned\")\n",
    "    \n",
    "    # ----- 4.7) 예시로 추론 테스트\n",
    "    test_text = \"애새끼가 초딩도 아니고 ㅋㅋㅋㅋ\"\n",
    "    model.eval()\n",
    "    \n",
    "    proc_text = preprocess_text(test_text)\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        proc_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=200,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        pred_label = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    if pred_label == 1:\n",
    "        print(f\"[결과] '{test_text}' => 비속어 판정\")\n",
    "    else:\n",
    "        print(f\"[결과] '{test_text}' => 정상 문장\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32820dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
